\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}

\begin{document}

\title{Machine Learning - Coursera\\
\large by Stanford University}

\maketitle
\newpage
\section{Linear Regression with one variable}

\subsection{Representation of hypothesis h}
 
 For a one variable Linear Reagression, the hypothesis function h is defined as : 
\begin{center}
	 $$h_\theta(x) = \theta_0 + \theta_1x$$
\end{center}

Where $x$ is the variable and [$\theta_0$, $\theta_1$] are the parameters of the linear regression model.

\subsection{Cost function J}
The purpose of this function is to select the best parameters $\theta_0$ and $\theta_1$ for the model. The best parameters are the ones that makes $h_\theta(x)$ the closest possible to $y$, $y$ being the value we want to predict.
The chosen cost function J is :
\begin{center}
	$$J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i = 1}^{m}(h_\theta(x^i) - y^i)^2$$
\end{center}
Where $m$ is the number of sample data we have. What that function computes is half the mean of the squared differences between the predicted values $h_\theta(x^i)$ and the actual values $y^i$.\\
The goal now is to minimise $J$ as to find the best parameters $\theta_0$ and $\theta_1$.

\subsection{Gradient descent algortithm}
\subsubsection{Defintion}

This alogrithm helps us to find a local minimum for the cost function $J(\theta)$. It works by \underline{simultaneously} updating the paramaters $\theta_0$ and $\theta_1$ following this equation :
\begin{center}
	$$\theta_j =  \theta_j - \alpha \frac{\partial}{\partial\theta_j}J(\theta_0, \theta_1)$$
\end{center}
Where $\alpha$ is called the learning rate.\\
Example of simultaneous update :

\begin{center}
	
	\begin{tabular}{l}
		
		$temp0 := \theta_0 =  \theta_0 - \alpha \frac{\partial}{\partial\theta_0}J(\theta_0, \theta_1)$\\
		$temp1 := \theta_1 =  \theta_1 - \alpha \frac{\partial}{\partial\theta_1}J(\theta_0, \theta_1)$\\
		$\theta_0 := temp0$\\
		$\theta_1 := temp1$
		
	\end{tabular}

\end{center}

\subsubsection{Application on one-variable linear regressgion}
The partial derivatives for the one-variable linear regression are as follow : 
$$\frac{\partial}{\partial\theta_0}J(\theta_0, \theta_1) = \frac{1}{m}\sum_{i = 1}^{m}(h_\theta(x^i) - y^i)^2$$
$$\frac{\partial}{\partial\theta_i}J(\theta_0, \theta_1) = \frac{1}{m}\sum_{i = 1}^{m}(h_\theta(x^i) - y^i)^2 \cdot x^i$$

So we repeat this next step until convergence :
$$\theta_0 = \theta_0 - \frac{\alpha}{m}\sum_{i = 1}^{m}(h_\theta(x^i) - y^i)^2$$
$$\theta_1 = \theta_1 - \frac{\alpha}{m}\sum_{i = 1}^{m}(h_\theta(x^i) - y^i)^2 \cdot x^i$$


\section{Multivariate Linear Regression}
\subsection{Notations}

$x^{(i)}_j$ is the value of the feature $j$ in the $i^{th}$ training example.\\
$x^{(i)}$ is the input (all the features) of the $i^{th}$ training example.\\
$m$ is the number of training examples.\\
$n$ is the number of features.

\subsection{Representation of hypothesis h}
For a Multivariate Linear Regression, the hypothesis function h is defined as : 
	$$h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + \cdots + \theta_n x_n$$
We can write that under this vectorized form :
$$h_\theta(x) = {\begin{bmatrix}\theta_{0} \theta_{1} \theta_{2} \cdots \theta_{n}\end{bmatrix}
\begin{bmatrix}
	x_{0} \\
	x_{1} \\
	x_{2} \\
	\vdots \\
	x_{n}
\end{bmatrix}} = \theta^Tx$$

By convention, $x_{0}$ is set equal to 1.\\
Moreover, the cost function is the same as the one-variate linear regression.

\subsection{Gradient Descent}
From what we saw in the previous chapter about gradient descent we obtain that equation for the multivariate linear regression :

$$\theta_j =  \theta_j - \alpha \frac{\partial}{\partial\theta_j}J(\theta_0, \theta_1) = \theta_j - \frac{\alpha}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})\cdot x^{(i)}_{j}$$
For $j$ from 0 to n.

\subsection{Feature Scaling}

We can speed up the gradient descent by having each of our input values in roughly the same range. That is because $\theta$ will descend quickly on small ranges. Ideally, we would like our input values to be inside those ranges :\\

$$-1 \leq x_{(i)} \leq 1$$
$$or$$
$$-0.5 \leq x_{(i)} \leq 0.5$$
\end{document}
